{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/banner.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting and Removing Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the world of data analysis, duplicate records are like uninvited guests at a party ‚Äì they show up unexpectedly and can cause quite a bit of chaos if not managed properly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/deduplication.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicate records are instances in a dataset where the same information appears more than once. These repetitions can occur for various reasons:\n",
    "\n",
    "- Data entry errors\n",
    "- System glitches during data collection\n",
    "- Merging datasets from multiple sources\n",
    "- User-generated duplicates (e.g., double-clicking a submit button)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîë **Key Concept:** Duplicate records are not always identical copies. They can be partial duplicates or near-duplicates, which we'll explore in more detail later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you're analyzing customer data, and you count John Doe twice because his information was accidentally entered twice. Now your analysis suggests you have more customers than you actually do. This simple example illustrates why detecting and handling duplicates is crucial:\n",
    "\n",
    "1. **Data Integrity:** Duplicates can skew your data, leading to incorrect conclusions.\n",
    "2. **Storage Efficiency:** Unnecessary duplicates waste storage space.\n",
    "3. **Processing Time:** Duplicate records increase the time needed for data processing and analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Pro Tip:** Always check for duplicates as part of your initial data cleaning process. It's easier to handle them early than to discover their impact later in your analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detecting duplicates might seem straightforward ‚Äì just look for identical rows, right? But in reality, it's often more complex:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Simple duplicate detection\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Name': ['John Smith', 'Jane Doe', 'John Smith', 'Jon Smith'],\n",
    "    'Age': [30, 25, 30, 30],\n",
    "    'City': ['New York', 'Boston', 'New York', 'New York']\n",
    "})\n",
    "\n",
    "print(df.duplicated().sum())  # Output: 1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we easily catch the exact duplicate. But what about 'Jon Smith'? It's likely a misspelling of 'John Smith' but won't be caught by simple duplicate detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î **Why This Matters:** Real-world data is messy. Names get misspelled, ages might be off by a year, or cities might be entered with slight variations. Effective duplicate detection needs to account for these real-world inconsistencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we delve deeper into duplicate detection, we'll explore:\n",
    "- Different types of duplicates you might encounter\n",
    "- Advanced methods for identifying duplicates, including fuzzy matching\n",
    "- Strategies for handling partial and near-duplicates\n",
    "- The impact of duplicates on your analysis\n",
    "- Best practices for maintaining clean, duplicate-free datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the end of this lecture, you'll be equipped with the knowledge and tools to tackle duplicate records effectively, ensuring your data analysis starts on a solid, clean foundation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Important Note:** Remember, the goal isn't always to remove all duplicates blindly. Sometimes, what appears to be a duplicate might be legitimate data. Always understand your data context before making decisions about duplicate removal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Types of Duplicates in Datasets](#toc1_)    \n",
    "  - [Exact Duplicates](#toc1_1_)    \n",
    "  - [Partial Duplicates](#toc1_2_)    \n",
    "  - [Near-Duplicates (Fuzzy Duplicates)](#toc1_3_)    \n",
    "  - [Cross-Column Duplicates](#toc1_4_)    \n",
    "  - [Semantic Duplicates](#toc1_5_)    \n",
    "  - [Time-Based Duplicates](#toc1_6_)    \n",
    "  - [Handling Different Types of Duplicates](#toc1_7_)    \n",
    "- [Methods for Identifying Duplicate Records](#toc2_)    \n",
    "  - [Exact Match Method](#toc2_1_)    \n",
    "  - [Subset Matching](#toc2_2_)    \n",
    "  - [Fuzzy Matching](#toc2_3_)    \n",
    "  - [Phonetic Matching](#toc2_4_)    \n",
    "  - [Distance-Based Methods](#toc2_5_)    \n",
    "  - [Machine Learning Approaches for Duplicate Detection](#toc2_6_)    \n",
    "    - [Supervised Learning Approach](#toc2_6_1_)    \n",
    "    - [Unsupervised Learning Approach](#toc2_6_2_)    \n",
    "    - [Hybrid Approach](#toc2_6_3_)    \n",
    "  - [Combining Methods](#toc2_7_)    \n",
    "- [Impact of Duplicates on Data Analysis](#toc3_)    \n",
    "  - [Statistical Distortions](#toc3_1_)    \n",
    "  - [Machine Learning Model Performance](#toc3_2_)    \n",
    "  - [Business Intelligence and Decision Making](#toc3_3_)    \n",
    "  - [Data Storage and Processing Efficiency](#toc3_4_)    \n",
    "  - [Mitigation Strategies](#toc3_5_)    \n",
    "- [Best Practices for Duplicate Detection and Removal](#toc4_)    \n",
    "  - [Understand Your Data](#toc4_1_)    \n",
    "  - [Implement a Comprehensive Detection Strategy](#toc4_2_)    \n",
    "  - [Prioritize Fields for Matching](#toc4_3_)    \n",
    "  - [Handle Different Data Types Appropriately](#toc4_4_)    \n",
    "  - [Standardize and Clean Data Before Duplicate Detection](#toc4_5_)    \n",
    "  - [Use Indexing for Large Datasets](#toc4_6_)    \n",
    "  - [Validate and Refine Your Approach](#toc4_7_)    \n",
    "  - [Decide on an Appropriate Removal Strategy](#toc4_8_)    \n",
    "  - [Implement Ongoing Duplicate Prevention](#toc4_9_)    \n",
    "  - [Document Your Process](#toc4_10_)    \n",
    "- [Summary](#toc5_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=2\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_'></a>[Types of Duplicates in Datasets](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When diving into the world of data duplication, it's crucial to understand that not all duplicates are created equal. Let's explore the various types of duplicates you might encounter in your datasets, each with its own set of challenges and solutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_'></a>[Exact Duplicates](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exact duplicates are the most straightforward type ‚Äì they're identical copies of a record, matching across all fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Doe</td>\n",
       "      <td>30</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jane Smith</td>\n",
       "      <td>25</td>\n",
       "      <td>Boston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John Doe</td>\n",
       "      <td>30</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Name  Age      City\n",
       "0    John Doe   30  New York\n",
       "1  Jane Smith   25    Boston\n",
       "2    John Doe   30  New York"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Name': ['John Doe', 'Jane Smith', 'John Doe'],\n",
    "    'Age': [30, 25, 30],\n",
    "    'City': ['New York', 'Boston', 'New York']\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Doe</td>\n",
       "      <td>30</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jane Smith</td>\n",
       "      <td>25</td>\n",
       "      <td>Boston</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Name  Age      City\n",
       "0    John Doe   30  New York\n",
       "1  Jane Smith   25    Boston"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîë **Key Concept:** Exact duplicates are often the easiest to detect and remove, but they're just the tip of the iceberg in real-world datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_'></a>[Partial Exact Duplicates](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial duplicates occur when some, but not all, fields match between records. These are trickier to identify and handle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Doe</td>\n",
       "      <td>30</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>John Doe</td>\n",
       "      <td>30</td>\n",
       "      <td>Boston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jane Smith</td>\n",
       "      <td>25</td>\n",
       "      <td>Boston</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Name  Age      City\n",
       "0    John Doe   30  New York\n",
       "1    John Doe   30    Boston\n",
       "2  Jane Smith   25    Boston"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Name': ['John Doe', 'John Doe', 'Jane Smith'],\n",
    "    'Age': [30, 30, 25],\n",
    "    'City': ['New York', 'Boston', 'Boston']\n",
    "})\n",
    "\n",
    "df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, 'John Doe' appears twice with the same age but different cities. Is this a duplicate or two different people with the same name and age? Using `drop_duplicates()` without specifying a subset will remove all duplicates, which might not be what you want. You can specify a subset of columns to remove duplicates based on those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Doe</td>\n",
       "      <td>30</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jane Smith</td>\n",
       "      <td>25</td>\n",
       "      <td>Boston</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Name  Age      City\n",
       "0    John Doe   30  New York\n",
       "2  Jane Smith   25    Boston"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates(subset=['Name', 'Age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_'></a>[Near-Duplicates (Fuzzy Duplicates)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Near-duplicates are records that are very similar but not exactly the same. They often result from data entry errors, different spellings, or slight variations in format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "- \"John Smith\", 30, New York\n",
    "- \"Jon Smyth\", 30, NY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Smith</td>\n",
       "      <td>30</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jon Smyth</td>\n",
       "      <td>30</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Name  Age      City\n",
       "0  John Smith   30  New York\n",
       "1   Jon Smyth   30        NY"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Name': ['John Smith', 'Jon Smyth'],\n",
    "    'Age': [30, 30],\n",
    "    'City': ['New York', 'NY']\n",
    "})\n",
    "df.drop_duplicates(subset=['Name', 'Age'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Pro Tip:** Detecting near-duplicates often requires fuzzy matching techniques, which we'll explore in later sections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_'></a>[Cross-Column Duplicates](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, the same information appears in different columns, creating a type of duplicate that's not immediately obvious.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Full_Name</th>\n",
       "      <th>First_Name</th>\n",
       "      <th>Last_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Doe</td>\n",
       "      <td>John</td>\n",
       "      <td>Doe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jane Smith</td>\n",
       "      <td>Jane</td>\n",
       "      <td>Smith</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Full_Name First_Name Last_Name\n",
       "0    John Doe       John       Doe\n",
       "1  Jane Smith       Jane     Smith"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Full_Name': ['John Doe', 'Jane Smith'],\n",
    "    'First_Name': ['John', 'Jane'],\n",
    "    'Last_Name': ['Doe', 'Smith']\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Full_Name</th>\n",
       "      <th>First_Name</th>\n",
       "      <th>Last_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Doe</td>\n",
       "      <td>John</td>\n",
       "      <td>Doe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jane Smith</td>\n",
       "      <td>Jane</td>\n",
       "      <td>Smith</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Full_Name First_Name Last_Name\n",
       "0    John Doe       John       Doe\n",
       "1  Jane Smith       Jane     Smith"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates(subset=['Full_Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the full name is duplicated across separate first and last name columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_5_'></a>[Semantic Duplicates](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic duplicates represent the same entity but with different representations. These are among the most challenging to detect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples:\n",
    "- \"IBM\" and \"International Business Machines\"\n",
    "- \"H2O\" and \"Water\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î **Why This Matters:** Identifying semantic duplicates often requires domain knowledge and can't be fully automated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_6_'></a>[Time-Based Duplicates](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In time-series data, you might encounter duplicates that are identical except for a timestamp:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Action</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>User1</td>\n",
       "      <td>Login</td>\n",
       "      <td>2023-05-01 10:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>User1</td>\n",
       "      <td>Login</td>\n",
       "      <td>2023-05-01 10:00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>User2</td>\n",
       "      <td>Logout</td>\n",
       "      <td>2023-05-01 11:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    User  Action            Timestamp\n",
       "0  User1   Login  2023-05-01 10:00:00\n",
       "1  User1   Login  2023-05-01 10:00:01\n",
       "2  User2  Logout  2023-05-01 11:00:00"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'User': ['User1', 'User1', 'User2'],\n",
    "    'Action': ['Login', 'Login', 'Logout'],\n",
    "    'Timestamp': ['2023-05-01 10:00:00', '2023-05-01 10:00:01', '2023-05-01 11:00:00']\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are these separate events or a system glitch creating duplicates?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_7_'></a>[Handling Different Types of Duplicates](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each type of duplicate requires a different approach:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Exact duplicates:** Often safe to remove automatically.\n",
    "2. **Partial duplicates:** Require careful consideration and possibly manual review.\n",
    "3. **Near-duplicates:** Need fuzzy matching techniques and threshold setting.\n",
    "4. **Cross-column duplicates:** May require data restructuring.\n",
    "5. **Semantic duplicates:** Often need domain-specific rules or machine learning approaches.\n",
    "6. **Time-based duplicates:** Require understanding of the data's temporal nature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Important Note:** The key to effective duplicate handling is to first identify what types of duplicates exist in your dataset, then develop a strategy tailored to each type.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By understanding these different types of duplicates, you're better equipped to tackle the complex task of data deduplication. In the next sections, we'll explore methods for identifying these various duplicate types and strategies for handling them effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_'></a>[Methods for Identifying Duplicate Records](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying duplicate records is a crucial step in data cleaning and preparation. Let's dive deeper into each method, exploring the tools and libraries that make these techniques possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_'></a>[Exact Match Method](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact match method is the most straightforward approach to finding duplicates. It identifies records that are identical across all fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Doe</td>\n",
       "      <td>30</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jane Smith</td>\n",
       "      <td>25</td>\n",
       "      <td>Boston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John Doe</td>\n",
       "      <td>30</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alice Johnson</td>\n",
       "      <td>35</td>\n",
       "      <td>Chicago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>John Doe</td>\n",
       "      <td>30</td>\n",
       "      <td>Los Angeles</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Name  Age         City\n",
       "0       John Doe   30     New York\n",
       "1     Jane Smith   25       Boston\n",
       "2       John Doe   30     New York\n",
       "3  Alice Johnson   35      Chicago\n",
       "4       John Doe   30  Los Angeles"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Name': ['John Doe', 'Jane Smith', 'John Doe', 'Alice Johnson', 'John Doe'],\n",
    "    'Age': [30, 25, 30, 35, 30],\n",
    "    'City': ['New York', 'Boston', 'New York', 'Chicago', 'Los Angeles']\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Duplicates:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John Doe</td>\n",
       "      <td>30</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Name  Age      City\n",
       "2  John Doe   30  New York"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify exact duplicates\n",
    "exact_duplicates = df[df.duplicated()]\n",
    "print(\"Exact Duplicates:\")\n",
    "exact_duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we're using Pandas, a powerful data manipulation library in Python. The `duplicated()` method returns a boolean Series indicating duplicate rows. When used with boolean indexing (`df[df.duplicated()]`), it returns the duplicate rows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîë **Key Concept:** Pandas' `duplicated()` method has a `keep` parameter. By default (`keep='first'`), it marks all but the first occurrence of a duplicate as True. You can also use `keep='last'` or `keep=False` to change this behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_'></a>[Subset Matching](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset matching allows you to focus on specific columns for identifying duplicates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Duplicates (Name and Age):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Doe</td>\n",
       "      <td>30</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John Doe</td>\n",
       "      <td>30</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>John Doe</td>\n",
       "      <td>30</td>\n",
       "      <td>Los Angeles</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Name  Age         City\n",
       "0  John Doe   30     New York\n",
       "2  John Doe   30     New York\n",
       "4  John Doe   30  Los Angeles"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify duplicates based on Name and Age\n",
    "subset_duplicates = df[df.duplicated(subset=['Name', 'Age'], keep=False)]\n",
    "\n",
    "print(\"Subset Duplicates (Name and Age):\")\n",
    "subset_duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `subset` parameter in `duplicated()` lets you specify which columns to consider. This is particularly useful when you have columns that might legitimately have duplicate values (like 'City') but want to focus on key identifying fields.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Pro Tip:** Use `keep=False` to identify all duplicate rows, not just subsequent occurrences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_'></a>[Fuzzy Matching](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuzzy matching is essential for detecting near-duplicates. It uses string similarity metrics to identify records that are close but not exactly the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: thefuzz in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (0.19.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install thefuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages/thefuzz/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from thefuzz import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio('John Doe', 'John D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.partial_ratio('John Doe', 'John D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2), (0, 4), (2, 4)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def fuzzy_dedupe(df, column, threshold=80):\n",
    "    duplicates = []\n",
    "    for (idx1, row1), (idx2, row2) in itertools.combinations(df.iterrows(), 2):\n",
    "        if fuzz.ratio(row1[column], row2[column]) >= threshold:\n",
    "            duplicates.append((idx1, idx2))\n",
    "    return duplicates\n",
    "\n",
    "fuzzy_dupes = fuzzy_dedupe(df, 'Name')\n",
    "fuzzy_dupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuzzy Duplicates:\n",
      "Potential duplicate:\n",
      "\n",
      "Name    John Doe\n",
      "Age           30\n",
      "City    New York\n",
      "Name: 0, dtype: object\n",
      "\n",
      "Name    John Doe\n",
      "Age           30\n",
      "City    New York\n",
      "Name: 2, dtype: object\n",
      "--------------------------------------------------\n",
      "Potential duplicate:\n",
      "\n",
      "Name    John Doe\n",
      "Age           30\n",
      "City    New York\n",
      "Name: 0, dtype: object\n",
      "\n",
      "Name       John Doe\n",
      "Age              30\n",
      "City    Los Angeles\n",
      "Name: 4, dtype: object\n",
      "--------------------------------------------------\n",
      "Potential duplicate:\n",
      "\n",
      "Name    John Doe\n",
      "Age           30\n",
      "City    New York\n",
      "Name: 2, dtype: object\n",
      "\n",
      "Name       John Doe\n",
      "Age              30\n",
      "City    Los Angeles\n",
      "Name: 4, dtype: object\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Fuzzy Duplicates:\")\n",
    "for idx1, idx2 in fuzzy_dupes:\n",
    "    print(\"Potential duplicate:\")\n",
    "    print()\n",
    "    print(df.loc[idx1])\n",
    "    print()\n",
    "    print(df.loc[idx2])\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we're using the `fuzzywuzzy` library, which provides various string matching algorithms. The `fuzz.ratio()` function calculates the Levenshtein distance ratio between two strings.\n",
    "\n",
    "- `itertools.combinations()` generates all possible pairs of rows.\n",
    "- We compare each pair and if the similarity ratio exceeds our threshold, we consider it a potential duplicate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î **Why This Matters:** Fuzzy matching can catch typos, slight variations in spelling, and other near-matches that exact matching would miss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_4_'></a>[Phonetic Matching](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phonetic matching is useful for catching duplicates that sound the same but are spelled differently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jellyfish in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install jellyfish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phonetic Duplicates:\n",
      "       Name  Age         City phonetic\n",
      "0  John Doe   30     New York     J530\n",
      "2  John Doh   30     New York     J530\n",
      "4  John Doe   30  Los Angeles     J530\n"
     ]
    }
   ],
   "source": [
    "import jellyfish\n",
    "\n",
    "# data with phonetic duplicates\n",
    "df = pd.DataFrame({\n",
    "    'Name': ['John Doe', 'Jane Smith', 'John Doh', 'Alice Johnson', 'John Doe'],\n",
    "    'Age': [30, 25, 30, 35, 30],\n",
    "    'City': ['New York', 'Boston', 'New York', 'Chicago', 'Los Angeles']\n",
    "})\n",
    "\n",
    "def phonetic_match(df, column):\n",
    "    df['phonetic'] = df[column].apply(jellyfish.soundex)\n",
    "    return df[df.duplicated(subset='phonetic', keep=False)].sort_values('phonetic')\n",
    "\n",
    "phonetic_dupes = phonetic_match(df, 'Name')\n",
    "print(\"Phonetic Duplicates:\")\n",
    "print(phonetic_dupes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using the `jellyfish` library here, which provides various phonetic encoding algorithms. Soundex is a phonetic algorithm that indexes names by sound, as pronounced in English.\n",
    "\n",
    "- `jellyfish.soundex()` converts names to their Soundex code.\n",
    "- We then use Pandas' `duplicated()` method on this new 'phonetic' column to find matches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_5_'></a>[Distance-Based Methods](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For numerical or categorical data, distance-based methods can be effective. These methods calculate the similarity or distance between records across multiple fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Doe</td>\n",
       "      <td>32</td>\n",
       "      <td>170</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jane Smith</td>\n",
       "      <td>26</td>\n",
       "      <td>165</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John Doe</td>\n",
       "      <td>29</td>\n",
       "      <td>170</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alice Johnson</td>\n",
       "      <td>31</td>\n",
       "      <td>168</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>John Doe</td>\n",
       "      <td>28</td>\n",
       "      <td>170</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Name  Age  Height  Weight\n",
       "0       John Doe   32     170      60\n",
       "1     Jane Smith   26     165      55\n",
       "2       John Doe   29     170      60\n",
       "3  Alice Johnson   31     168      58\n",
       "4       John Doe   28     170      60"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def distance_based_duplicates(df, columns, threshold=5):\n",
    "    subset = df[columns]\n",
    "    dist_matrix = pdist(subset.values, metric='euclidean')\n",
    "    dist_matrix = squareform(dist_matrix)\n",
    "    print(f'dist_matrix shape: {dist_matrix.shape}')\n",
    "    print(dist_matrix)\n",
    "    print('-' * 50)\n",
    "    duplicate_pairs = np.argwhere((dist_matrix > 0) & (dist_matrix < threshold))\n",
    "    return duplicate_pairs\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Name': ['John Doe', 'Jane Smith', 'John Doe', 'Alice Johnson', 'John Doe'],\n",
    "    'Age': [32, 26, 29, 31, 28],\n",
    "    'Height': [170, 165, 170, 168, 170],\n",
    "    'Weight': [60, 55, 60, 58, 60]\n",
    "})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dist_matrix shape: (5, 5)\n",
      "[[0.         9.2736185  3.         3.         4.        ]\n",
      " [9.2736185  0.         7.68114575 6.55743852 7.34846923]\n",
      " [3.         7.68114575 0.         3.46410162 1.        ]\n",
      " [3.         6.55743852 3.46410162 0.         4.12310563]\n",
      " [4.         7.34846923 1.         4.12310563 0.        ]]\n",
      "--------------------------------------------------\n",
      "Distance-Based Duplicates:\n",
      "2 duplicates found\n",
      "       Name  Age  Height  Weight\n",
      "2  John Doe   29     170      60\n",
      "4  John Doe   28     170      60\n",
      "--------------------------------------------------\n",
      "       Name  Age  Height  Weight\n",
      "4  John Doe   28     170      60\n",
      "2  John Doe   29     170      60\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Only use numeric columns for distance calculation\n",
    "numeric_columns = ['Age','Height', 'Weight']\n",
    "dist_dupes = distance_based_duplicates(df, numeric_columns, threshold=3)\n",
    "\n",
    "print(\"Distance-Based Duplicates:\")\n",
    "if len(dist_dupes) > 0:\n",
    "    print(f'{len(dist_dupes)} duplicates found')\n",
    "    for pair in dist_dupes:\n",
    "        print(df.iloc[pair])\n",
    "        print(\"-\" * 50)\n",
    "else:\n",
    "    print(\"No duplicates found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we're using SciPy's distance functions:\n",
    "- `pdist()` computes pairwise distances between observations.\n",
    "- `squareform()` converts the distance matrix to a square form.\n",
    "- We then use NumPy to find pairs of points that are close but not identical.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Important Note:** For this method to work well, your numerical features should be on a similar scale. Consider normalizing your data first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_6_'></a>[Machine Learning Approaches for Duplicate Detection](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning can be a powerful tool for detecting duplicates, especially in complex datasets. We'll explore both supervised and unsupervised approaches using a sample dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create our sample dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>City</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Smith</td>\n",
       "      <td>29</td>\n",
       "      <td>New York</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jane Doe</td>\n",
       "      <td>28</td>\n",
       "      <td>Boston</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jon Smyth</td>\n",
       "      <td>31</td>\n",
       "      <td>New York</td>\n",
       "      <td>51000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jane Doe</td>\n",
       "      <td>26</td>\n",
       "      <td>Boston</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>John Smyth</td>\n",
       "      <td>28</td>\n",
       "      <td>NY</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Janet Doe</td>\n",
       "      <td>26</td>\n",
       "      <td>Bostan</td>\n",
       "      <td>61000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Name  Age      City  Salary\n",
       "0  John Smith   29  New York   50000\n",
       "1    Jane Doe   28    Boston   60000\n",
       "2   Jon Smyth   31  New York   51000\n",
       "3    Jane Doe   26    Boston   60000\n",
       "4  John Smyth   28        NY   50000\n",
       "5   Janet Doe   26    Bostan   61000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from thefuzz import fuzz\n",
    "\n",
    "# Create a sample dataset\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'Name': ['John Smith', 'Jane Doe', 'Jon Smyth', 'Jane Doe', 'John Smyth', 'Janet Doe'],\n",
    "    'Age': [29, 28, 31, 26, 28, 26],\n",
    "    'City': ['New York', 'Boston', 'New York', 'Boston', 'NY', 'Bostan'],\n",
    "    'Salary': [50000, 60000, 51000, 60000, 50000, 61000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Sample Dataset:\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_6_1_'></a>[Supervised Learning Approach](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a supervised approach, we treat duplicate detection as a binary classification problem. We'll start by creating a labeled dataset of record pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pair_features(left, right):\n",
    "    \"\"\"Create features that capture the similarity between two records\"\"\"\n",
    "    features = {}\n",
    "    features['name_similarity'] = fuzz.ratio(left['Name'], right['Name']) / 100.0\n",
    "    features['age_diff'] = abs(left['Age'] - right['Age'])\n",
    "    features['city_similarity'] = fuzz.ratio(left['City'], right['City']) / 100.0\n",
    "    features['salary_diff'] = abs(left['Salary'] - right['Salary'])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Create a sample dataset\n",
    "fake = Faker()\n",
    "num_records = 20\n",
    "num_unique_values = 3\n",
    "\n",
    "names = [fake.name() for _ in range(num_unique_values)]\n",
    "age = [fake.random_int(min=20, max=40) for _ in range(num_unique_values)]\n",
    "cities = [fake.city() for _ in range(num_unique_values)]\n",
    "salaries = [fake.random_int(min=30000, max=100000) for _ in range(num_unique_values)]\n",
    "\n",
    "names = [random.choice(names) for _ in range(num_records)]\n",
    "age = [random.choice(age) for _ in range(num_records)]\n",
    "cities = [random.choice(cities) for _ in range(num_records)]\n",
    "salaries = [random.choice(salaries) for _ in range(num_records)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_similarity</th>\n",
       "      <th>age_diff</th>\n",
       "      <th>city_similarity</th>\n",
       "      <th>salary_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.30</td>\n",
       "      <td>3</td>\n",
       "      <td>0.22</td>\n",
       "      <td>32623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>32623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.23</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>15979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.23</td>\n",
       "      <td>1</td>\n",
       "      <td>0.17</td>\n",
       "      <td>32623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.23</td>\n",
       "      <td>1</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0.23</td>\n",
       "      <td>1</td>\n",
       "      <td>0.17</td>\n",
       "      <td>16644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>32623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>1.00</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>15979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0.08</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.08</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>15979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     name_similarity  age_diff  city_similarity  salary_diff\n",
       "0               0.30         3             0.22        32623\n",
       "1               0.23         0             0.17        32623\n",
       "2               0.23         3             1.00        15979\n",
       "3               0.23         1             0.17        32623\n",
       "4               0.23         1             0.17            0\n",
       "..               ...       ...              ...          ...\n",
       "185             0.23         1             0.17        16644\n",
       "186             1.00         0             0.17        32623\n",
       "187             1.00         3             1.00        15979\n",
       "188             0.08         2             1.00            0\n",
       "189             0.08         1             1.00        15979\n",
       "\n",
       "[190 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create all possible pairs of records\n",
    "pairs = []\n",
    "labels = []\n",
    "for i in range(num_records):\n",
    "    left = {\n",
    "        'Name': names[i],\n",
    "        'Age': age[i],\n",
    "        'City': cities[i],\n",
    "        'Salary': salaries[i]\n",
    "    }\n",
    "    for j in range(i+1, num_records):\n",
    "        right = {\n",
    "            'Name': names[j],\n",
    "            'Age': age[j],\n",
    "            'City': cities[j],\n",
    "            'Salary': salaries[j]\n",
    "        }\n",
    "        pairs.append(create_pair_features(left, right))\n",
    "        # Label as duplicate if names are very similar and age difference is small\n",
    "        is_duplicate = (fuzz.ratio(left['Name'], right['Name']) > 80) and (abs(left['Age'] - right['Age']) <= 1)\n",
    "        labels.append(int(is_duplicate))\n",
    "\n",
    "X = pd.DataFrame(pairs)\n",
    "y = pd.Series(labels)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Supervised Learning Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        27\n",
      "           1       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        38\n",
      "   macro avg       1.00      1.00      1.00        38\n",
      "weighted avg       1.00      1.00      1.00        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"\\nSupervised Learning Results:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on all pairs\n",
    "all_pairs_pred = clf.predict(X_test)\n",
    "all_pairs_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name_similarity    1.00\n",
      "age_diff           1.00\n",
      "city_similarity    0.22\n",
      "salary_diff        0.00\n",
      "Name: 133, dtype: float64\n",
      "--------------------------------------------------\n",
      "name_similarity        1.00\n",
      "age_diff               0.00\n",
      "city_similarity        0.22\n",
      "salary_diff        15979.00\n",
      "Name: 8, dtype: float64\n",
      "--------------------------------------------------\n",
      "name_similarity        1.00\n",
      "age_diff               1.00\n",
      "city_similarity        0.17\n",
      "salary_diff        32623.00\n",
      "Name: 15, dtype: float64\n",
      "--------------------------------------------------\n",
      "name_similarity        1.00\n",
      "age_diff               1.00\n",
      "city_similarity        0.17\n",
      "salary_diff        16644.00\n",
      "Name: 160, dtype: float64\n",
      "--------------------------------------------------\n",
      "name_similarity        1.00\n",
      "age_diff               0.00\n",
      "city_similarity        0.22\n",
      "salary_diff        15979.00\n",
      "Name: 67, dtype: float64\n",
      "--------------------------------------------------\n",
      "name_similarity    1.00\n",
      "age_diff           1.00\n",
      "city_similarity    0.22\n",
      "salary_diff        0.00\n",
      "Name: 182, dtype: float64\n",
      "--------------------------------------------------\n",
      "name_similarity        1.00\n",
      "age_diff               0.00\n",
      "city_similarity        0.17\n",
      "salary_diff        32623.00\n",
      "Name: 186, dtype: float64\n",
      "--------------------------------------------------\n",
      "name_similarity        1.00\n",
      "age_diff               0.00\n",
      "city_similarity        0.17\n",
      "salary_diff        32623.00\n",
      "Name: 91, dtype: float64\n",
      "--------------------------------------------------\n",
      "name_similarity    1.0\n",
      "age_diff           1.0\n",
      "city_similarity    1.0\n",
      "salary_diff        0.0\n",
      "Name: 38, dtype: float64\n",
      "--------------------------------------------------\n",
      "name_similarity        1.0\n",
      "age_diff               1.0\n",
      "city_similarity        1.0\n",
      "salary_diff        32623.0\n",
      "Name: 171, dtype: float64\n",
      "--------------------------------------------------\n",
      "name_similarity        1.00\n",
      "age_diff               1.00\n",
      "city_similarity        0.17\n",
      "salary_diff        16644.00\n",
      "Name: 141, dtype: float64\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, pred in enumerate(all_pairs_pred):\n",
    "    if pred == 1:\n",
    "        print(X_test.iloc[i])\n",
    "        print('-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîë **Key Concept:** The feature engineering step is crucial here. We create features that capture the similarity between pairs of records.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_6_2_'></a>[Unsupervised Learning Approach](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an unsupervised approach, we use similarity scores to identify potential duplicates without labeled data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Potential Duplicates (Unsupervised):\n",
      "Record 0 and Record 2 might be duplicates (similarity: 0.65)\n",
      "Name      John Smith\n",
      "Age               29\n",
      "City        New York\n",
      "Salary         50000\n",
      "Name: 0, dtype: object\n",
      "Name      Jon Smyth\n",
      "Age              31\n",
      "City       New York\n",
      "Salary        51000\n",
      "Name: 2, dtype: object\n",
      "\n",
      "Record 1 and Record 3 might be duplicates (similarity: 0.83)\n",
      "Name      Jane Doe\n",
      "Age             28\n",
      "City        Boston\n",
      "Salary       60000\n",
      "Name: 1, dtype: object\n",
      "Name      Jane Doe\n",
      "Age             26\n",
      "City        Boston\n",
      "Salary       60000\n",
      "Name: 3, dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def unsupervised_duplicates(df, threshold=0.6):\n",
    "    # Prepare the data\n",
    "    numeric_cols = ['Age', 'Salary']\n",
    "    categorical_cols = ['City', 'Name']\n",
    "\n",
    "    # One-hot encode categorical variables\n",
    "    df_encoded = pd.get_dummies(df, columns=categorical_cols)\n",
    "\n",
    "    # Standardize numeric features\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = pd.DataFrame(scaler.fit_transform(df_encoded[numeric_cols]), columns=numeric_cols)\n",
    "\n",
    "    # Combine scaled numeric and one-hot encoded features\n",
    "    df_prepared = pd.concat([df_scaled, df_encoded.drop(columns=numeric_cols)], axis=1)\n",
    "\n",
    "    # Compute similarity matrix\n",
    "    sim_matrix = cosine_similarity(df_prepared)\n",
    "\n",
    "    # Find potential duplicates\n",
    "    potential_duplicates = []\n",
    "    for i in range(len(df)):\n",
    "        for j in range(i+1, len(df)):\n",
    "            if sim_matrix[i][j] > threshold:\n",
    "                potential_duplicates.append((i, j, sim_matrix[i][j]))\n",
    "\n",
    "    return potential_duplicates\n",
    "\n",
    "duplicates = unsupervised_duplicates(df)\n",
    "print(\"\\nPotential Duplicates (Unsupervised):\")\n",
    "for i, j, sim in duplicates:\n",
    "    print(f\"Record {i} and Record {j} might be duplicates (similarity: {sim:.2f})\")\n",
    "    print(df.iloc[i])\n",
    "    print(df.iloc[j])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Pro Tip:** The choice of similarity metric and threshold can greatly affect results. Experiment with different options like Jaccard similarity for categorical data or Euclidean distance for numeric data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_6_3_'></a>[Hybrid Approach](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, a hybrid approach often works best. Here's a simplified example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Potential Duplicates (Hybrid Approach):\n",
      "Record 0 and Record 4 might be duplicates\n",
      "Name      John Smith\n",
      "Age               29\n",
      "City        New York\n",
      "Salary         50000\n",
      "Name: 0, dtype: object\n",
      "Name      John Smyth\n",
      "Age               28\n",
      "City              NY\n",
      "Salary         50000\n",
      "Name: 4, dtype: object\n",
      "\n",
      "Record 3 and Record 5 might be duplicates\n",
      "Name      Jane Doe\n",
      "Age             26\n",
      "City        Boston\n",
      "Salary       60000\n",
      "Name: 3, dtype: object\n",
      "Name      Janet Doe\n",
      "Age              26\n",
      "City         Bostan\n",
      "Salary        61000\n",
      "Name: 5, dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def hybrid_duplicate_detection(df, unsupervised_threshold=0.25, supervised_threshold=0.5):\n",
    "    # Step 1: Use unsupervised method to identify potential duplicates\n",
    "    potential_dupes = unsupervised_duplicates(df, threshold=unsupervised_threshold)\n",
    "\n",
    "    # Step 2: Create features for these potential duplicates\n",
    "    X = []\n",
    "    for i, j, _ in potential_dupes:\n",
    "        X.append(create_pair_features(df.iloc[i], df.iloc[j]))\n",
    "    X = pd.DataFrame(X)\n",
    "\n",
    "    # Step 3: Use the supervised model to classify these potential duplicates\n",
    "    probas = clf.predict_proba(X)[:, 1]  # Probability of being a duplicate\n",
    "\n",
    "    # Step 4: Filter based on the supervised model's confidence\n",
    "    final_duplicates = [(potential_dupes[i][0], potential_dupes[i][1])\n",
    "                        for i, proba in enumerate(probas) if proba > supervised_threshold]\n",
    "\n",
    "    return final_duplicates\n",
    "\n",
    "hybrid_dupes = hybrid_duplicate_detection(df)\n",
    "print(\"\\nPotential Duplicates (Hybrid Approach):\")\n",
    "for i, j in hybrid_dupes:\n",
    "    print(f\"Record {i} and Record {j} might be duplicates\")\n",
    "    print(df.iloc[i])\n",
    "    print(df.iloc[j])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î **Why This Matters:** The hybrid approach combines the strengths of both supervised and unsupervised methods. It uses unsupervised learning to cast a wide net for potential duplicates, then applies the more precise supervised model to filter these candidates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Important Note:** The effectiveness of these approaches heavily depends on the quality of your features and the characteristics of your data. Always validate your results and be prepared to iterate on your approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example provides a comprehensive look at how machine learning can be applied to duplicate detection. It starts with a simple dataset and demonstrates supervised, unsupervised, and hybrid approaches. In a real-world scenario, you would likely need to refine these methods further, possibly incorporating more sophisticated feature engineering or trying different machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_7_'></a>[Combining Methods](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, a combination of these methods often yields the best results. You might start with exact matching, then use fuzzy matching on the remaining records, and finally apply domain-specific rules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All Detected Duplicates:\n",
      "         Name Age      City Salary phonetic\n",
      "0  John Smith  29  New York  50000     J525\n",
      "2   Jon Smyth  31  New York  51000     J525\n",
      "4  John Smyth  28        NY  50000     J525\n",
      "1    Jane Doe  28    Boston  60000     J530\n",
      "3    Jane Doe  26    Boston  60000     J530\n"
     ]
    }
   ],
   "source": [
    "# Pseudocode for a combined approach\n",
    "def identify_duplicates(df):\n",
    "    exact_dupes = df[df.duplicated(keep=False)]\n",
    "    remaining = df.drop(exact_dupes.index)\n",
    "\n",
    "    fuzzy_dupes = pd.DataFrame(columns=df.columns)\n",
    "    for col in ['Name', 'City']:\n",
    "        fuzzy_dupes = pd.concat([fuzzy_dupes,\n",
    "                                 remaining[remaining.index.isin(fuzzy_dedupe(remaining, col))]])\n",
    "\n",
    "    remaining = remaining.drop(fuzzy_dupes.index)\n",
    "    phonetic_dupes = phonetic_match(remaining, 'Name')\n",
    "\n",
    "    return pd.concat([exact_dupes, fuzzy_dupes, phonetic_dupes])\n",
    "\n",
    "all_duplicates = identify_duplicates(df)\n",
    "print(\"\\nAll Detected Duplicates:\")\n",
    "print(all_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This combined approach leverages the strengths of each method:\n",
    "1. It starts with exact matching to catch obvious duplicates.\n",
    "2. Then applies fuzzy matching to catch near-duplicates.\n",
    "3. Finally, uses phonetic matching to catch any remaining sound-alike duplicates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By understanding and applying these various methods, you can create a robust duplicate detection system tailored to your specific dataset and requirements. Remember, the key is to understand your data and choose the methods that best fit your use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_'></a>[Impact of Duplicates on Data Analysis](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicate records in datasets are more than just a nuisance; they can significantly skew your analysis and lead to erroneous conclusions. Understanding the impact of duplicates is crucial for maintaining data integrity and ensuring the reliability of your analytical results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_'></a>[Statistical Distortions](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîë **Key Concept:** Duplicates can artificially inflate or deflate statistical measures, leading to misinterpretation of data.\n",
    "\n",
    "1. **Descriptive Statistics**:\n",
    "   - Measures of central tendency (mean, median) can be shifted.\n",
    "   - Variance and standard deviation may be underestimated.\n",
    "\n",
    "2. **Frequency Distributions**:\n",
    "   - Duplicates can exaggerate the frequency of certain values, distorting histograms and frequency tables.\n",
    "\n",
    "3. **Correlations**:\n",
    "   - Duplicate records can strengthen or weaken correlations between variables, potentially leading to false insights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Pro Tip:** Always check for and handle duplicates before calculating summary statistics or performing correlation analyses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_'></a>[Machine Learning Model Performance](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicates can have a significant impact on machine learning models:\n",
    "\n",
    "1. **Overfitting**:\n",
    "   - If duplicates are present in both training and test sets, models may appear to perform better than they actually do.\n",
    "\n",
    "2. **Biased Learning**:\n",
    "   - Models may give undue importance to duplicated instances, leading to biased predictions.\n",
    "\n",
    "3. **Data Leakage**:\n",
    "   - In time-series data, duplicates can cause future information to leak into the past, leading to unrealistic model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Important Note:** Be especially cautious with duplicates when performing cross-validation or time-series forecasting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_3_'></a>[Business Intelligence and Decision Making](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The presence of duplicates can lead to flawed business insights:\n",
    "\n",
    "1. **Customer Analytics**:\n",
    "   - Duplicate customer records can lead to overestimation of customer base or misunderstanding of customer behavior.\n",
    "\n",
    "2. **Financial Reporting**:\n",
    "   - Duplicate transactions can inflate revenue figures or distort expense reports.\n",
    "\n",
    "3. **Inventory Management**:\n",
    "   - Duplicate product entries can lead to incorrect stock levels and inefficient supply chain management.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î **Why This Matters:** Inaccurate business intelligence can lead to poor strategic decisions and misallocation of resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_4_'></a>[Data Storage and Processing Efficiency](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While perhaps less critical than analytical impacts, duplicates also affect system performance:\n",
    "\n",
    "1. **Storage Costs**:\n",
    "   - Unnecessary duplicates waste storage space, which can be significant in large datasets.\n",
    "\n",
    "2. **Processing Time**:\n",
    "   - Duplicate records increase the time needed for data processing and analysis.\n",
    "\n",
    "3. **Data Transfer**:\n",
    "   - In distributed systems, duplicates increase the volume of data that needs to be transferred between nodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_5_'></a>[Mitigation Strategies](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address the impact of duplicates:\n",
    "\n",
    "1. Implement robust duplicate detection methods as part of your data pipeline.\n",
    "2. Regularly audit your data for duplicates, especially before critical analyses.\n",
    "3. When duplicates are found, carefully consider whether to merge, delete, or flag them based on your specific use case.\n",
    "4. Document your duplicate handling procedures for transparency and reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean with duplicate: 3.0\n",
      "Mean without duplicate: 3.0\n"
     ]
    }
   ],
   "source": [
    "# Example of how duplicates can affect basic statistics\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Value': [1, 2, 3, 3, 4, 5]  # Note the duplicate '3'\n",
    "})\n",
    "\n",
    "print(f\"Mean with duplicate: {df['Value'].mean()}\")\n",
    "print(f\"Mean without duplicate: {df['Value'].drop_duplicates().mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By understanding and addressing the impact of duplicates, you can ensure more accurate analyses, better model performance, and more reliable business insights. Remember, the goal isn't always to remove all duplicates blindly, but to handle them in a way that maintains the integrity of your data and the accuracy of your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_'></a>[Best Practices for Duplicate Detection and Removal](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effective duplicate detection and removal is crucial for maintaining data quality. Here are some best practices to guide you through this process:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_1_'></a>[Understand Your Data](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîë **Key Concept:** Before you start, gain a thorough understanding of your dataset.\n",
    "\n",
    "- Identify key fields that should be unique (e.g., customer ID, transaction number).\n",
    "- Understand which fields might legitimately have duplicates.\n",
    "- Consider the business context of your data to inform your duplicate detection strategy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_2_'></a>[Implement a Comprehensive Detection Strategy](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine multiple methods for robust duplicate detection:\n",
    "\n",
    "- **Exact Matching**: Start with identifying exact duplicates across all relevant fields.\n",
    "- **Fuzzy Matching**: Use techniques like Levenshtein distance or Soundex for near-matches.\n",
    "- **Rule-Based Approaches**: Develop domain-specific rules for your data.\n",
    "- **Machine Learning**: For complex datasets, consider supervised or unsupervised ML approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Pro Tip:** No single method is perfect. A combination of approaches often yields the best results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_3_'></a>[Prioritize Fields for Matching](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all fields are equally important for identifying duplicates:\n",
    "\n",
    "- Focus on fields that are most likely to uniquely identify a record (e.g., name, date of birth, address).\n",
    "- Assign weights to different fields based on their importance in determining a duplicate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Pseudocode for weighted field matching\n",
    "def is_duplicate(record1, record2, weights):\n",
    "    total_weight = sum(weights.values())\n",
    "    match_score = sum(weights[field] for field in weights if record1[field] == record2[field])\n",
    "    return match_score / total_weight > 0.8  # 80% match threshold\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_4_'></a>[Handle Different Data Types Appropriately](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different data types require different duplicate detection approaches:\n",
    "\n",
    "- **Strings**: Use fuzzy matching techniques.\n",
    "- **Numbers**: Consider using range-based matching or rounding.\n",
    "- **Dates**: Standardize format before comparison.\n",
    "- **Categorical Data**: Consider semantic similarities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_5_'></a>[Standardize and Clean Data Before Duplicate Detection](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î **Why This Matters:** Standardization can significantly improve duplicate detection accuracy.\n",
    "\n",
    "- Normalize text (e.g., convert to lowercase, remove punctuation).\n",
    "- Standardize formats (e.g., phone numbers, addresses).\n",
    "- Handle missing values consistently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_6_'></a>[Use Indexing for Large Datasets](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large datasets, comparing every record with every other is computationally expensive:\n",
    "\n",
    "- Implement blocking or indexing strategies to reduce the number of comparisons.\n",
    "- Group records by a common attribute (e.g., first letter of last name) before detailed comparison.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_7_'></a>[Validate and Refine Your Approach](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicate detection is often an iterative process:\n",
    "\n",
    "- Start with a small subset of your data to test your approach.\n",
    "- Manually review a sample of detected duplicates to assess accuracy.\n",
    "- Refine your methods based on false positives and false negatives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_8_'></a>[Decide on an Appropriate Removal Strategy](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Important Note:** Removing duplicates isn't always straightforward.\n",
    "\n",
    "- Consider whether to keep the first occurrence, last occurrence, or merge information.\n",
    "- In some cases, flagging duplicates without removal might be more appropriate.\n",
    "- Document your removal strategy for transparency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Example of different removal strategies\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'ID': [1, 2, 2, 3], 'Value': [10, 20, 25, 30]})\n",
    "\n",
    "print(\"Keep first:\")\n",
    "print(df.drop_duplicates(subset='ID', keep='first'))\n",
    "\n",
    "print(\"\\nKeep last:\")\n",
    "print(df.drop_duplicates(subset='ID', keep='last'))\n",
    "\n",
    "print(\"\\nMerge (sum):\")\n",
    "print(df.groupby('ID').sum().reset_index())\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_9_'></a>[Implement Ongoing Duplicate Prevention](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't just remove duplicates ‚Äì prevent them:\n",
    "\n",
    "- Implement real-time duplicate checking for data entry systems.\n",
    "- Use unique constraints in databases where appropriate.\n",
    "- Regularly audit your data for new duplicates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_10_'></a>[Document Your Process](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintain clear documentation of your duplicate detection and removal process:\n",
    "\n",
    "- Detail the methods used, thresholds set, and decisions made.\n",
    "- Keep logs of removed or merged records for auditing purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By following these best practices, you can develop a robust and effective approach to duplicate detection and removal. Remember, the goal is not just to remove duplicates, but to do so in a way that maintains data integrity and supports accurate analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_'></a>[Summary](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we conclude our exploration of duplicate detection and removal, let's recap the key points and reflect on their importance in the data processing pipeline. Here are the key takeaways:\n",
    "\n",
    "1. Duplicate records can significantly impact data analysis, machine learning models, and business decisions.\n",
    "2. Various types of duplicates exist, from exact matches to fuzzy duplicates and semantic duplicates.\n",
    "3. A multi-faceted approach combining different methods often yields the best results in duplicate detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've covered a range of techniques for identifying duplicates:\n",
    "\n",
    "- Exact matching for straightforward cases\n",
    "- Fuzzy matching for handling near-duplicates and typos\n",
    "- Phonetic algorithms for sound-alike matches\n",
    "- Machine learning approaches for complex patterns and large datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Pro Tip:** The choice of method depends on your specific data characteristics and use case. Don't hesitate to combine methods for more robust results. Handling duplicates is critical in many ways since:\n",
    "- Unhandled duplicates can lead to skewed analytics, biased machine learning models, and flawed business decisions.\n",
    "- Effective duplicate management improves data quality, enhances system performance, and ensures regulatory compliance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've outlined several best practices, including:\n",
    "\n",
    "1. Understanding your data before starting\n",
    "2. Implementing a comprehensive detection strategy\n",
    "3. Standardizing and cleaning data\n",
    "4. Validating and refining your approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As data volumes continue to grow and data sources diversify, the challenge of duplicate detection will only become more complex. Stay informed about emerging techniques, especially in the realms of machine learning and big data processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mastering duplicate detection is a crucial skill for any data professional. It's not just about cleaning data; it's about ensuring the integrity and reliability of the insights derived from that data. By applying the methods and best practices we've discussed, you'll be well-equipped to tackle duplicate detection challenges in your data projects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, the goal isn't perfection, but continuous improvement in your data quality. Each dataset is unique, and your approach should be tailored to your specific needs and constraints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you move forward, keep experimenting with different techniques, stay curious about new methods, and always keep the end goal in mind: high-quality, reliable data that drives accurate insights and informed decisions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
